{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рубежный контроль №2. Методы машинного обучения\n",
    "\n",
    "**Студент:** Седойкин Г.С., ИУ5-22М\n",
    "\n",
    "### Задание.\n",
    "***\n",
    "Необходимо решить задачу классификации текстов на основе любого выбранного Вами датасета (кроме примера, который рассматривался в лекции). Классификация может быть бинарной или многоклассовой. Целевой признак из выбранного Вами датасета может иметь любой физический смысл, примером является задача анализа тональности текста.\n",
    "\n",
    "Необходимо сформировать два варианта векторизации признаков - на основе CountVectorizer и на основе TfidfVectorizer.\n",
    "\n",
    "В качестве классификаторов необходимо использовать два классификатора по варианту для Вашей группы (`RandomForestClassifier`,`Complement Naive Bayes (CNB)`)  \n",
    "\n",
    "Для каждого метода необходимо оценить качество классификации. Сделайте вывод о том, какой вариант векторизации признаков в паре с каким классификатором показал лучшее качество. \n",
    "\n",
    "### Выполнение задания.\n",
    "***\n",
    "Задания буду выполнять на [датасете](https://github.com/bhargaviparanjape/clickbait/tree/master/dataset) из статьи \"Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media\".  \n",
    "Датасет предполагает бинарную классификацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет состоит из 2х файлов (кликбейт и не кликбейт заголовки). Загрузим их, установим значение целевого признак, объеденим датасеты в единый DataFrame и перемешаем строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header_text</th>\n",
       "      <th>is_clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Michigan, Bank Lends Little of Its Bailout ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Four dead, more than a million in U.S. without...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Wyoming, Debate Rages Over Elk Feeding Program</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bryant and Lakers Return to the N.B.A. Finals</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This Baby's Reaction To Hearing About How She ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>When You Binge-Watch \"Making A Murderer\" And T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>Schiphol airliner crash blamed on altimeter fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>Radiohead Release Rejected Bond Theme Song And...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Chernobyl Taking a Toll on Invertebrates Too</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>8 Things No One Tells Guys With Body Image Anx...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             header_text  is_clickbait\n",
       "0      In Michigan, Bank Lends Little of Its Bailout ...             0\n",
       "1      Four dead, more than a million in U.S. without...             0\n",
       "2      In Wyoming, Debate Rages Over Elk Feeding Program             0\n",
       "3          Bryant and Lakers Return to the N.B.A. Finals             0\n",
       "4      This Baby's Reaction To Hearing About How She ...             1\n",
       "...                                                  ...           ...\n",
       "31995  When You Binge-Watch \"Making A Murderer\" And T...             1\n",
       "31996  Schiphol airliner crash blamed on altimeter fa...             0\n",
       "31997  Radiohead Release Rejected Bond Theme Song And...             1\n",
       "31998       Chernobyl Taking a Toll on Invertebrates Too             0\n",
       "31999  8 Things No One Tells Guys With Body Image Anx...             1\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clickbait = pd.read_csv(\"../data/clickbait_data.txt\", sep = \"\\n\\n\", engine=\"python\", names = [\"header_text\"], header = None)\n",
    "data_clickbait[\"is_clickbait\"] = [1] * data_clickbait.shape[0]\n",
    "data_non_clickbait = pd.read_csv(\"../data/non_clickbait_data.txt\", sep = \"\\n\\n\", engine=\"python\", names = [\"header_text\"], header = None)\n",
    "data_non_clickbait[\"is_clickbait\"] = [0] * data_non_clickbait.shape[0]\n",
    "dataset = data_clickbait.append(data_non_clickbait)\n",
    "dataset = dataset.sample(frac = 1, random_state = 100)\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем общий словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 22761\n"
     ]
    }
   ],
   "source": [
    "vocabulary = CountVectorizer().fit(dataset[\"header_text\"].tolist()).vocabulary_\n",
    "print(\"Размер словаря: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модели используя кросс-валидацию и разные векторизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '000th': 2, '00s': 3, '01': 4,\n",
      "                            '04': 5, '05': 6, '08': 7, '08m': 8, '09': 9,\n",
      "                            '10': 10, '100': 11, '1000': 12, '10000th': 13,\n",
      "                            '1000blackgirls': 14, '100k': 15, '100m': 16,\n",
      "                            '100th': 17, '100ºf': 18, '101': 19, '101st': 20,\n",
      "                            '102': 21, '103': 22, '104': 23, '105': 24,\n",
      "                            '106': 25, '107': 26, '108': 27, '109': 28,\n",
      "                            '109th': 29, ...})\n",
      "Модель для классификации - RandomForestClassifier()\n",
      "Accuracy = 0.9566250135875828\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '000th': 2, '00s': 3, '01': 4,\n",
      "                            '04': 5, '05': 6, '08': 7, '08m': 8, '09': 9,\n",
      "                            '10': 10, '100': 11, '1000': 12, '10000th': 13,\n",
      "                            '1000blackgirls': 14, '100k': 15, '100m': 16,\n",
      "                            '100th': 17, '100ºf': 18, '101': 19, '101st': 20,\n",
      "                            '102': 21, '103': 22, '104': 23, '105': 24,\n",
      "                            '106': 25, '107': 26, '108': 27, '109': 28,\n",
      "                            '109th': 29, ...})\n",
      "Модель для классификации - ComplementNB()\n",
      "Accuracy = 0.9718125370554792\n",
      "===========================\n",
      "\n",
      "\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '000th': 2, '00s': 3, '01': 4,\n",
      "                            '04': 5, '05': 6, '08': 7, '08m': 8, '09': 9,\n",
      "                            '10': 10, '100': 11, '1000': 12, '10000th': 13,\n",
      "                            '1000blackgirls': 14, '100k': 15, '100m': 16,\n",
      "                            '100th': 17, '100ºf': 18, '101': 19, '101st': 20,\n",
      "                            '102': 21, '103': 22, '104': 23, '105': 24,\n",
      "                            '106': 25, '107': 26, '108': 27, '109': 28,\n",
      "                            '109th': 29, ...})\n",
      "Модель для классификации - RandomForestClassifier()\n",
      "Accuracy = 0.9575937245257542\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '000th': 2, '00s': 3, '01': 4,\n",
      "                            '04': 5, '05': 6, '08': 7, '08m': 8, '09': 9,\n",
      "                            '10': 10, '100': 11, '1000': 12, '10000th': 13,\n",
      "                            '1000blackgirls': 14, '100k': 15, '100m': 16,\n",
      "                            '100th': 17, '100ºf': 18, '101': 19, '101st': 20,\n",
      "                            '102': 21, '103': 22, '104': 23, '105': 24,\n",
      "                            '106': 25, '107': 26, '108': 27, '109': 28,\n",
      "                            '109th': 29, ...})\n",
      "Модель для классификации - ComplementNB()\n",
      "Accuracy = 0.96787500189044\n",
      "===========================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for vectorizer in vectorizers_list:\n",
    "        for classifier in classifiers_list:\n",
    "            pipeline = Pipeline([(\"vectorizer\", vectorizer), (\"classifier\", classifier)])\n",
    "            score = cross_val_score(pipeline, dataset[\"header_text\"], dataset[\"is_clickbait\"], \n",
    "                                    scoring = 'accuracy', cv = 3, n_jobs = -1).mean()\n",
    "            print('Векторизация - {}'.format(vectorizer))\n",
    "            print('Модель для классификации - {}'.format(classifier))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')\n",
    "        print(\"\\n\")\n",
    "\n",
    "vectorizers_list = [CountVectorizer(vocabulary = vocabulary), TfidfVectorizer(vocabulary = vocabulary)]\n",
    "classifiers_list = [RandomForestClassifier(), ComplementNB()]\n",
    "\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "***\n",
    "Наилучший результат был получен для векторизатора - `CountVectorizer` с классификатром `ComplementNB()`: *0.9718125370554792*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
