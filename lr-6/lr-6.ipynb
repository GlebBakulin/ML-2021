{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №6. Классификация текста.\n",
    "### Задание.\n",
    "***\n",
    "Для произвольного набора данных, предназначенного для классификации текстов, решите задачу классификации текста двумя способами:\n",
    "\n",
    "Способ 1. На основе CountVectorizer или TfidfVectorizer.  \n",
    "Способ 2. На основе моделей word2vec или Glove или fastText.  \n",
    "Сравните качество полученных моделей. \n",
    "\n",
    "Для первого способа буду использовать `TfidfVectorizer`, для второго `Word2Vec`. Классификатор - `KNeighborsClassifier`.\n",
    "\n",
    "### Выполнение задания.\n",
    "***\n",
    "Задания буду выполнять на [датасете](https://github.com/bhargaviparanjape/clickbait/tree/master/dataset) из статьи \"Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media\".  \n",
    "Датасет предполагает бинарную классификацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет состоит из 2х файлов (кликбейт и не кликбейт заголовки). Загрузим их, установим значение целевого признак, объеденим датасеты в единый DataFrame и перемешаем строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header_text</th>\n",
       "      <th>is_clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Michigan, Bank Lends Little of Its Bailout ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Four dead, more than a million in U.S. without...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Wyoming, Debate Rages Over Elk Feeding Program</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bryant and Lakers Return to the N.B.A. Finals</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This Baby's Reaction To Hearing About How She ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>When You Binge-Watch \"Making A Murderer\" And T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>Schiphol airliner crash blamed on altimeter fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>Radiohead Release Rejected Bond Theme Song And...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Chernobyl Taking a Toll on Invertebrates Too</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>8 Things No One Tells Guys With Body Image Anx...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             header_text  is_clickbait\n",
       "0      In Michigan, Bank Lends Little of Its Bailout ...             0\n",
       "1      Four dead, more than a million in U.S. without...             0\n",
       "2      In Wyoming, Debate Rages Over Elk Feeding Program             0\n",
       "3          Bryant and Lakers Return to the N.B.A. Finals             0\n",
       "4      This Baby's Reaction To Hearing About How She ...             1\n",
       "...                                                  ...           ...\n",
       "31995  When You Binge-Watch \"Making A Murderer\" And T...             1\n",
       "31996  Schiphol airliner crash blamed on altimeter fa...             0\n",
       "31997  Radiohead Release Rejected Bond Theme Song And...             1\n",
       "31998       Chernobyl Taking a Toll on Invertebrates Too             0\n",
       "31999  8 Things No One Tells Guys With Body Image Anx...             1\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clickbait = pd.read_csv(\"../data/clickbait_data.txt\", sep = \"\\n\\n\", engine=\"python\", names = [\"header_text\"], header = None)\n",
    "data_clickbait[\"is_clickbait\"] = [1] * data_clickbait.shape[0]\n",
    "data_non_clickbait = pd.read_csv(\"../data/non_clickbait_data.txt\", sep = \"\\n\\n\", engine=\"python\", names = [\"header_text\"], header = None)\n",
    "data_non_clickbait[\"is_clickbait\"] = [0] * data_non_clickbait.shape[0]\n",
    "dataset = data_clickbait.append(data_non_clickbait)\n",
    "dataset = dataset.sample(frac = 1, random_state = 100)\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataset[\"header_text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация на основе TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 22761\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vocabulary = tfidf_vectorizer.fit(documents) # Формируем набор признаков и параметры idf\n",
    "print(\"Число признаков: {}\".format(len(vocabulary.vocabulary_)))\n",
    "documents_vectorized = tfidf_vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый документ, векторизованный (не нулевые признаки): [0.38686554025791037, 0.3186643872558275, 0.395177358289709, 0.1288209792126749, 0.2891144884412702, 0.4776795401471119, 0.31294434961282225, 0.38248152507739863, 0.14155161385070417]\n",
      "Первый документ, оригинальный: In Michigan, Bank Lends Little of Its Bailout Funds\n"
     ]
    }
   ],
   "source": [
    "print(\"Первый документ, векторизованный (не нулевые признаки): {}\"\n",
    "      .format(str([i for i in documents_vectorized.todense()[0].getA1() if i > 0])))\n",
    "print(\"Первый документ, оригинальный: {}\".format(documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним классификацию на с помощью `KNeighborsClassifier` классификатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: = 0.9414061561326256\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(Pipeline([(\"vectorizer\", tfidf_vectorizer), (\"classifier\", KNeighborsClassifier())]), \n",
    "                        dataset[\"header_text\"], dataset[\"is_clickbait\"], scoring = 'accuracy', \n",
    "                        cv = 3, n_jobs = -1).mean()\n",
    "print('accuracy: = {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация на основе word2vec\n",
    "Токенизация и стемминг документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/adminu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "word_corpus = list()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "for line in documents:\n",
    "    line = re.sub(\"[^a-z]\",\" \", line.strip().lower()) # Избавляемся от не a-z символов\n",
    "    words = tokenizer.tokenize(line)\n",
    "    words = [stemmer.stem(word) for word in words if not word in stop_words]\n",
    "    word_corpus.append(words)\n",
    "assert(len(documents) == len(word_corpus) and \"Число документов изменилось\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели word2vec ([документация](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.94 s, sys: 28.5 ms, total: 4.97 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%time w2v_model = word2vec.Word2Vec(word_corpus, vector_size = 100, workers = 4, sample=1e-3) #min_count = 10, window=10,\n",
    "word_vectors = w2v_model.wv # Сохраним \"словарь\", полученный в результате обучения: \"слово:вектор\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset.shapeПротестируем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('helicopt', 0.9975600838661194), ('suspect', 0.9974792003631592), ('blast', 0.9974472522735596), ('explos', 0.9970493912696838), ('afghanistan', 0.996931791305542), ('fire', 0.9965904355049133), ('pakistan', 0.9965630769729614), ('afghan', 0.9964554905891418), ('plane', 0.9964352250099182), ('car', 0.9964020848274231)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar(positive = [\"soldier\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19343631,  0.17365696,  0.4686207 ,  0.04921315,  0.07405844,\n",
       "       -0.48390976, -0.00412363,  0.6372408 , -0.13001125, -0.14527614,\n",
       "       -0.23287095, -0.38185102, -0.02535676,  0.4315425 ,  0.1468047 ,\n",
       "       -0.10774412, -0.16411917, -0.35768422, -0.06804183, -0.7009506 ,\n",
       "        0.17280552,  0.10430055,  0.17514649, -0.188508  ,  0.0101337 ,\n",
       "        0.21406531, -0.13777924, -0.58364576, -0.26376018, -0.10030827,\n",
       "        0.4823668 ,  0.17824182, -0.27964368,  0.01551333, -0.04145198,\n",
       "        0.18587731, -0.00849575, -0.4707291 , -0.41801643, -0.5621453 ,\n",
       "        0.07018226, -0.13372253,  0.20581684, -0.08340184,  0.13442515,\n",
       "       -0.41398698, -0.50646794, -0.10498024,  0.11391275,  0.3029909 ,\n",
       "       -0.01619712, -0.27982628, -0.25633758,  0.07829376, -0.3871074 ,\n",
       "        0.1535437 ,  0.2678513 , -0.01037473, -0.24756695,  0.1298363 ,\n",
       "        0.21516818, -0.01351387, -0.2605288 , -0.0341438 , -0.18796273,\n",
       "        0.23165384, -0.15837623, -0.10025492, -0.1547997 ,  0.07070192,\n",
       "       -0.22622806,  0.26712275,  0.26131773, -0.06880721,  0.08620366,\n",
       "       -0.02405495,  0.11083637,  0.09558525, -0.36985457,  0.11683636,\n",
       "       -0.22546044, -0.04296049, -0.302572  ,  0.4662439 ,  0.06049803,\n",
       "        0.1113041 ,  0.2627964 ,  0.21143055,  0.2576516 , -0.05904451,\n",
       "        0.11309849,  0.16087443, -0.00593441,  0.07859407,  0.33659485,\n",
       "        0.31946486,  0.306354  , -0.61170983,  0.15809083,  0.34338346],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[\"soldier\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним классификацию на с помощью `KNeighborsClassifier` классификатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: = 0.660999814761789\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word_vectors):\n",
    "        self.word_vectors = word_vectors\n",
    "        self.vector_size = word_vectors.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Возвращает усредненный вектор от векторов входящих в документ слов (для отсутствующих слов - 0-ой вектор)\n",
    "        '''\n",
    "        return np.array([np.mean([self.word_vectors[word] for word in words \n",
    "                                  if word in self.word_vectors]\n",
    "                                  or [np.zeros(self.vector_size)], axis=0)\n",
    "                                  for words in X])\n",
    "\n",
    "score = cross_val_score(Pipeline([(\"vectorizer\", EmbeddingVectorizer(word_vectors)), (\"classifier\", KNeighborsClassifier())]), \n",
    "                        dataset[\"header_text\"], dataset[\"is_clickbait\"], scoring = 'accuracy', \n",
    "                        cv = 3, n_jobs = -1).mean()\n",
    "print('accuracy: = {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "***\n",
    "Наилучший результат был получен с `TfidfVectorizer` (точность **0.941** против **0.661** у `Word2Vec`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
